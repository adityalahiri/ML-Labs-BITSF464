{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Training_Set.csv')\n",
    "test_df = pd.read_csv('Testing_Set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separating X and y\n",
    "\n",
    "y = train_df['count']\n",
    "X = train_df.drop(['count'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train Test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_scale = ['season', 'holiday', 'workingday', 'weather', 'temp',\n",
    "       'atemp', 'humidity', 'windspeed', 'casual', 'registered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scalar = RobustScaler()\n",
    "X_train[to_scale] = scalar.fit_transform(X_train[to_scale])\n",
    "X_test[to_scale] = scalar.transform(X_test[to_scale])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(['datetime'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.75, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=1000, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingRegressor(learning_rate=0.75,n_estimators=1000)\n",
    "gb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arrayslayer/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_test.drop(['datetime'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "\n",
    "#TODO\n",
    "\n",
    "gb = GradientBoostingRegressor(n_estimators=500,learning_rate=0.75)        #Initialize the classifier object\n",
    "\n",
    "parameters = {'n_estimators':[500,600,800],'learning_rate':[0.5,0.75,1.0,1.25],'max_depth':[4,6,8,10]}#Dictionary of parameters\n",
    "\n",
    "scorer = make_scorer(mean_squared_log_error)         #Initialize the scorer using make_scorer\n",
    "\n",
    "grid_obj = GridSearchCV(gb,parameters)         #Initialize a GridSearchCV object with above parameters,scorer and classifier\n",
    "\n",
    "grid_fit = grid_obj.fit(X_train,y_train)        #Fit the gridsearch object with X_train,y_train\n",
    "\n",
    "best_gb = grid_fit.best_estimator_         #Get the best estimator. For this, check documentation of GridSearchCV object\n",
    "\n",
    "#y_pred = best_clf.predict(X_test)\n",
    "\n",
    "\n",
    "#rmsle = np.sqrt(mean_squared_log_error(y_test, y_pred))       #Calculate accuracy for unoptimized model\n",
    "\n",
    "#print(\" score on optimized model:{}\".format(rmsle))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.5, loss='ls', max_depth=4, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=800, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_gb.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016651323555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "y_pred = gb.predict(X_test)\n",
    "print(np.sqrt(mean_squared_log_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.5, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=600, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_gb.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 31058.68857956\n",
      "Iteration 2, loss = 7203.04555196\n",
      "Iteration 3, loss = 1471.15144694\n",
      "Iteration 4, loss = 672.63157117\n",
      "Iteration 5, loss = 441.24780221\n",
      "Iteration 6, loss = 267.58716742\n",
      "Iteration 7, loss = 155.21762981\n",
      "Iteration 8, loss = 90.58267592\n",
      "Iteration 9, loss = 55.14463001\n",
      "Iteration 10, loss = 38.17262468\n",
      "Iteration 11, loss = 28.05928781\n",
      "Iteration 12, loss = 21.58388784\n",
      "Iteration 13, loss = 17.35519094\n",
      "Iteration 14, loss = 13.89642359\n",
      "Iteration 15, loss = 11.76007537\n",
      "Iteration 16, loss = 10.17006560\n",
      "Iteration 17, loss = 9.16694318\n",
      "Iteration 18, loss = 8.13578488\n",
      "Iteration 19, loss = 7.40162312\n",
      "Iteration 20, loss = 7.03130657\n",
      "Iteration 21, loss = 6.49343735\n",
      "Iteration 22, loss = 5.91057313\n",
      "Iteration 23, loss = 5.51158945\n",
      "Iteration 24, loss = 5.24922102\n",
      "Iteration 25, loss = 4.83015350\n",
      "Iteration 26, loss = 4.61302389\n",
      "Iteration 27, loss = 4.45364803\n",
      "Iteration 28, loss = 4.11633776\n",
      "Iteration 29, loss = 3.89048920\n",
      "Iteration 30, loss = 3.70791556\n",
      "Iteration 31, loss = 3.55577223\n",
      "Iteration 32, loss = 3.38199730\n",
      "Iteration 33, loss = 3.29170051\n",
      "Iteration 34, loss = 3.17645237\n",
      "Iteration 35, loss = 3.18386748\n",
      "Iteration 36, loss = 3.00287121\n",
      "Iteration 37, loss = 2.84670254\n",
      "Iteration 38, loss = 2.68822519\n",
      "Iteration 39, loss = 2.61460930\n",
      "Iteration 40, loss = 2.44795982\n",
      "Iteration 41, loss = 2.45622515\n",
      "Iteration 42, loss = 2.44184754\n",
      "Iteration 43, loss = 2.46784556\n",
      "Iteration 44, loss = 2.20271465\n",
      "Iteration 45, loss = 2.17481336\n",
      "Iteration 46, loss = 2.06796468\n",
      "Iteration 47, loss = 2.02259021\n",
      "Iteration 48, loss = 2.04316688\n",
      "Iteration 49, loss = 1.91114925\n",
      "Iteration 50, loss = 1.85224076\n",
      "Iteration 51, loss = 1.85235493\n",
      "Iteration 52, loss = 1.82707125\n",
      "Iteration 53, loss = 1.80417472\n",
      "Iteration 54, loss = 1.75400444\n",
      "Iteration 55, loss = 1.60611173\n",
      "Iteration 56, loss = 1.63869307\n",
      "Iteration 57, loss = 1.68174809\n",
      "Iteration 58, loss = 1.55252077\n",
      "Iteration 59, loss = 1.55650925\n",
      "Iteration 60, loss = 1.47530962\n",
      "Iteration 61, loss = 1.45618406\n",
      "Iteration 62, loss = 1.42185687\n",
      "Iteration 63, loss = 1.40719760\n",
      "Iteration 64, loss = 1.40037342\n",
      "Iteration 65, loss = 1.52321975\n",
      "Iteration 66, loss = 1.30856390\n",
      "Iteration 67, loss = 1.27214807\n",
      "Iteration 68, loss = 1.21666954\n",
      "Iteration 69, loss = 1.19214108\n",
      "Iteration 70, loss = 1.17120186\n",
      "Iteration 71, loss = 1.24197968\n",
      "Iteration 72, loss = 1.22995149\n",
      "Iteration 73, loss = 1.09515537\n",
      "Iteration 74, loss = 1.09185073\n",
      "Iteration 75, loss = 1.02995244\n",
      "Iteration 76, loss = 1.06044124\n",
      "Iteration 77, loss = 1.07411120\n",
      "Iteration 78, loss = 1.03323519\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "reg = MLPRegressor(hidden_layer_sizes=(100,200,300,100),verbose = True)\n",
    "reg.fit(X=X_train,y=y_train)\n",
    "y_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(base_estimator=DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best'),\n",
       "         learning_rate=1.0, loss='linear', n_estimators=500,\n",
       "         random_state=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "\n",
    "dtree = DecisionTreeRegressor()\n",
    "clf = AdaBoostRegressor(base_estimator=dtree,n_estimators = 500,learning_rate=1.6)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[to_scale] = scalar.transform(test_df[to_scale])\n",
    "dt = test_df['datetime']\n",
    "test_df.drop(['datetime'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_final_test_reg = reg.predict(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final_test_gb = gb.predict(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_final = (y_final_test_ada + y_final_test_reg)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_df = pd.DataFrame(data=y_final_test_gb,index=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans_df.to_csv('ans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
