{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "Today's lab will involve analyzing an unlabeled dataset and being able to cluster similar datapoints, and generate meaningful results from such data. It involves a greater level of data exploration and visualization, as well as clever feature extraction and engineering. \n",
    "\n",
    "*****************************************************************\n",
    "Today we will be clustering customers into segments based on their spending trends, so that a wholesale distributor can understand what kind of customer he/she is dealing with. This is an active machine learning problem in the e-commerce community, but we will be looking at a simplified version of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Region</th>\n",
       "      <th>Fresh</th>\n",
       "      <th>Milk</th>\n",
       "      <th>Grocery</th>\n",
       "      <th>Frozen</th>\n",
       "      <th>Detergents_Paper</th>\n",
       "      <th>Delicassen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12669</td>\n",
       "      <td>9656</td>\n",
       "      <td>7561</td>\n",
       "      <td>214</td>\n",
       "      <td>2674</td>\n",
       "      <td>1338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>7057</td>\n",
       "      <td>9810</td>\n",
       "      <td>9568</td>\n",
       "      <td>1762</td>\n",
       "      <td>3293</td>\n",
       "      <td>1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6353</td>\n",
       "      <td>8808</td>\n",
       "      <td>7684</td>\n",
       "      <td>2405</td>\n",
       "      <td>3516</td>\n",
       "      <td>7844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13265</td>\n",
       "      <td>1196</td>\n",
       "      <td>4221</td>\n",
       "      <td>6404</td>\n",
       "      <td>507</td>\n",
       "      <td>1788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>22615</td>\n",
       "      <td>5410</td>\n",
       "      <td>7198</td>\n",
       "      <td>3915</td>\n",
       "      <td>1777</td>\n",
       "      <td>5185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel  Region  Fresh  Milk  Grocery  Frozen  Detergents_Paper  Delicassen\n",
       "0        2       3  12669  9656     7561     214              2674        1338\n",
       "1        2       3   7057  9810     9568    1762              3293        1776\n",
       "2        2       3   6353  8808     7684    2405              3516        7844\n",
       "3        1       3  13265  1196     4221    6404               507        1788\n",
       "4        2       3  22615  5410     7198    3915              1777        5185"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata for this dataset is as below:\n",
    "Attribute Information:\n",
    "\n",
    "1) FRESH: annual spending (m.u.) on fresh products (Continuous)\n",
    "\n",
    "2) MILK: annual spending (m.u.) on milk products (Continuous)\n",
    "\n",
    "3) GROCERY: annual spending (m.u.)on grocery products (Continuous)\n",
    "\n",
    "4) FROZEN: annual spending (m.u.)on frozen products (Continuous)\n",
    "\n",
    "5) DETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous)\n",
    "\n",
    "6) DELICATESSEN: annual spending (m.u.)on and delicatessen products (Continuous)\n",
    "\n",
    "7) CHANNEL: customers Channel - Horeca (Hotel/Restaurant/Cafe) or Retail channel (Nominal)\n",
    "\n",
    "8) REGION: customers Region- Lisnon, Oporto or Other (Nominal) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Data Exploration\n",
    "\n",
    "Q1. Print the attributes of the dataset, and standard values(overall description) like mean, standard deviation, etc. to get a better idea of the attribute distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Print 3 to 5 random data points from the dataset, and try and understand what kinds of spenders they can be(e.g. a luxurious spender or a tight-budget guy). This can help you influence your model selection later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Unsupervised learning is all about understanding correlations between different attributes. Plot a scatter matrix using pandas between all attributes. If you wish, you can also plot a correlation heatmap. Analyze which attributes are more correlated and which aren't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "sns.set_style('white')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Plot a facet grid which compares 2 attributes with each other based on region. For example, Grocery vs Frozen based on region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Data Preprocessing\n",
    "\n",
    "Sometimes the data is still hard to map out despite visualizing it. It may require some scaling and preprocessing before the visualizations make sense.\n",
    "\n",
    "Q5. Apply a logarithmic transformation on the data, except for attributes Channel and Region(because that wouldn't make sense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "log_data = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Plot the scatter matrix again on the transformed data.Do you see any difference, any emerging correlations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Feature scaling is important in unsupervised learning particularly because it largely impacts the performance of clustering algorithms like K-means, and any algorithm which involves 'distance' calculation. If you wish, apply any scaling on the data other than logarithmic scaling below(you can try removing logarithmic scaling altogether):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Remove outliers from the transformed data using Tukey's method. However, since we do not want to remove too many points, view the points which are outliers with respect to each attribute, and then remove those which occur in too many attributes' outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "outliers = []  #These are the indices of the points that you want to remove\n",
    "cleaned_data = None  #Remove the above manually selected outlier points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Feature Transformation\n",
    "\n",
    "Once all the above is done, we need to generate features that encompass the distributions of independent attributes. We do this by applying PCA(Principal Component Analysis). What this transformation does is that it creates dimensions in the direction of maximum variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. We will now work only on the continuous data, so on all fields except Channel and Region, apply PCA on the data using pca.transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "#TODO\n",
    "continuous_data = None #Create a dataframe with only the continuous attributes.\n",
    "\n",
    "pca = None    #Apply PCA by fitting the data with an arbitrary number of dimensions(your choice)\n",
    "\n",
    "reduced_continuous_data = None   #Transform the continuous data\n",
    "\n",
    "df_reduced = None   #Create a dataframe from the reduced data, rename the dimensions D1,D2....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pca_results(good_data, pca):\n",
    "\n",
    "    # Dimension indexing\n",
    "    dimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n",
    "\n",
    "    # PCA components\n",
    "    components = pd.DataFrame(np.round(pca.components_, 4), columns = good_data.keys())\n",
    "    components.index = dimensions\n",
    "\n",
    "    # PCA explained variance\n",
    "    ratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n",
    "    variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n",
    "    variance_ratios.index = dimensions\n",
    "\n",
    "    # Create a bar plot visualization\n",
    "    fig, ax = plt.subplots(figsize = (14,8))\n",
    "\n",
    "    # Plot the feature weights as a function of the components\n",
    "    components.plot(ax = ax, kind = 'bar');\n",
    "    ax.set_ylabel(\"Feature Weights\")\n",
    "    ax.set_xticklabels(dimensions, rotation=0)\n",
    "\n",
    "\n",
    "    # Display the explained variance ratios\n",
    "    for i, ev in enumerate(pca.explained_variance_ratio_):\n",
    "        ax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Explained Variance\\n          %.4f\"%(ev))\n",
    "\n",
    "    # Return a concatenated DataFrame\n",
    "    return pd.concat([variance_ratios, components], axis = 1)\n",
    "\n",
    "pca_results(continuous_data,pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe the relative weights of each original feature in each new dimension/feature.\n",
    "\n",
    "Now, we can begin the process of clustering and learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. Create a scatter plot between pca dimensions 1 and 2, which are the most important features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def biplot(good_data, reduced_data, pca):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (14,8))\n",
    "    # scatterplot of the reduced data    \n",
    "    ax.scatter(x=reduced_data.loc[:, 'D1'], y=reduced_data.loc[:, 'D2'], \n",
    "        facecolors='b', edgecolors='b', s=70, alpha=0.5)\n",
    "    \n",
    "    feature_vectors = pca.components_.T\n",
    "\n",
    "    # we use scaling factors to make the arrows easier to see\n",
    "    arrow_size, text_pos = 7.0, 8.0,\n",
    "\n",
    "    # projections of the original features\n",
    "    for i, v in enumerate(feature_vectors):\n",
    "        ax.arrow(0, 0, arrow_size*v[0], arrow_size*v[1], \n",
    "                  head_width=0.2, head_length=0.2, linewidth=2, color='red')\n",
    "        ax.text(v[0]*text_pos, v[1]*text_pos, good_data.columns[i], color='black', \n",
    "                 ha='center', va='center', fontsize=18)\n",
    "\n",
    "    ax.set_xlabel(\"D1\", fontsize=14)\n",
    "    ax.set_ylabel(\"D2\", fontsize=14)\n",
    "    ax.set_title(\"PC plane with original feature projections.\", fontsize=16);\n",
    "    return ax\n",
    "\n",
    "biplot(continuous_data,df_reduced,pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows the projection of the original features along the new components. It becomes easier to understand how the clustering will be done in the future. For example, you can literally observe the possibility that someone who buys less milk and more delicatessen will be in one category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore another dimensionality reduction technique called t-SNE(t-distributed stochastic neighbor embeddings). It is a probabilistic reduction technique which is also used for visualization of high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11. Now, rerun the code cell for Q19, and choose the number of PCA dimensions greater than 2. Apply t-SNE on the continuous reduced data(check out:http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "#TODO\n",
    "\n",
    "tsne = None   #Instantiate t-SNE, set number of features to 2\n",
    "\n",
    "X_tsne = None #Fit and transform the continuous reduced features \n",
    "\n",
    "#Create a scatter plot on X_tsne\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can observe how higher dimensional data has been mapped down to 2 dimensions. \n",
    "\n",
    "Q12.(Optional) You can choose to apply either both PCA and t-SNE, or just one of them before proceeding. You can analyze the difference in performances later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Clustering\n",
    "\n",
    "We will now proceed to cluster the data. \n",
    "\n",
    "Q13. Pick any clustering algorithm(for starters, KMeans or Gaussian Mixture Model from sklearn). Check this link for more, but read carefully on whether it is applicable to such a problem: http://scikit-learn.org/stable/modules/clustering.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "clusterer = None  #Apply the clustering algorithm on the reduced training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q14. Generate predictions on the test set. Print the centers of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "preds = None\n",
    "\n",
    "centers = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q15. The metric we will use for evaluating clustering performance is silhouette score:http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html. Print the silhouette score on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "score = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "\n",
    "def cluster_results(reduced_data, preds, centers):\n",
    "\n",
    "    predictions = pd.DataFrame(preds, columns = ['Cluster'])\n",
    "    plot_data = pd.concat([predictions, reduced_data], axis = 1)\n",
    "\n",
    "    # Generate the cluster plot\n",
    "    fig, ax = plt.subplots(figsize = (14,8))\n",
    "\n",
    "    # Color map\n",
    "    cmap = cm.get_cmap('gist_rainbow')\n",
    "\n",
    "    # Color the points based on assigned cluster\n",
    "    for i, cluster in plot_data.groupby('Cluster'):   \n",
    "        cluster.plot(ax = ax, kind = 'scatter', x = 'D1', y = 'D2', \\\n",
    "                    color = cmap((i)*1.0/(len(centers)-1)), label = 'Cluster %i'%(i), s=30);\n",
    "\n",
    "    # Plot centers with indicators\n",
    "    for i, c in enumerate(centers):\n",
    "        ax.scatter(x = c[0], y = c[1], color = 'white', edgecolors = 'black', \\\n",
    "                   alpha = 1, linewidth = 2, marker = 'o', s=200);\n",
    "        ax.scatter(x = c[0], y = c[1], marker='$%d$'%(i), alpha = 1, s=100);\n",
    "\n",
    "    # Set plot title\n",
    "    ax.set_title(\"Cluster Learning on PCA-Reduced Data - Centroids Marked by Number\\nTransformed Sample Data Marked by Black Cross\");\n",
    "    \n",
    "cluster_results(df_reduced,preds,centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you can see how the clusters are split. Try changing the number of clusters to explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Data Inverse-Transformation\n",
    "\n",
    "Q16. Apply inverse transform on the cluster centers to obtain their true values. First apply pca.inverse_transform, then apply inverse transform on whatever log scaling/any other scaling you have performed. Print true values of the centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "log_centers = None\n",
    "\n",
    "true_centers = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q17.You have now successfully clustered the data into customer segments. To improve your performance in clustering, try using hyperparameter tuning on the algorithm,finding optimal number of clusters. Print your silhouette score after this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
